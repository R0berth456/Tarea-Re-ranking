{
  "cells": [
    {
      "metadata": {
        "id": "67e1446dc081a1fd"
      },
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 10: Re-ranking\n",
        "\n",
        "**Objetivo:** Implementar y evaluar un pipeline de Recuperación de Información en dos etapas, y analizar el impacto del re-ranking en la calidad del ranking.\n",
        "\n",
        "## Parte 1. Preparación del corpus\n",
        "\n",
        "* Cargar el corpus (documentos/pasajes).\n",
        "* Cargar las consultas (queries).\n",
        "* Cargar qrels (relevancia)."
      ],
      "id": "67e1446dc081a1fd"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beir"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jGZAefYlcQW5"
      },
      "id": "jGZAefYlcQW5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2026-01-12T02:47:52.579212Z",
          "start_time": "2026-01-12T02:47:52.173076Z"
        },
        "id": "initial_id"
      },
      "source": [
        "from beir import util\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-01-12T02:49:48.781159Z",
          "start_time": "2026-01-12T02:49:46.373362Z"
        },
        "id": "d364fd79098022cc"
      },
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"scifact\"\n",
        "DATA_DIR = \"../data/beir_datasets\"\n",
        "url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{DATASET_NAME}.zip\"\n",
        "util.download_and_unzip(url, DATA_DIR)"
      ],
      "id": "d364fd79098022cc",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-01-12T02:50:51.080480Z",
          "start_time": "2026-01-12T02:50:51.013933Z"
        },
        "id": "b68c3819fb71637d"
      },
      "cell_type": "code",
      "source": [
        "dataset_path = DATA_DIR + \"/\" + DATASET_NAME\n",
        "corpus, queries, qrels = GenericDataLoader(dataset_path).load(split=\"test\")"
      ],
      "id": "b68c3819fb71637d",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-01-12T02:51:31.962555Z",
          "start_time": "2026-01-12T02:51:31.936122Z"
        },
        "id": "3de5cc139813595"
      },
      "cell_type": "code",
      "source": [
        "df_corpus = (\n",
        "    pd.DataFrame.from_dict(corpus, orient=\"index\")\n",
        "      .reset_index()\n",
        "      .rename(columns={\"index\": \"doc_id\"})\n",
        ")\n",
        "\n",
        "df_corpus"
      ],
      "id": "3de5cc139813595",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-01-12T02:52:04.525430Z",
          "start_time": "2026-01-12T02:52:04.501833Z"
        },
        "id": "33099ccdf60f6a8b"
      },
      "cell_type": "code",
      "source": [
        "df_queries = (\n",
        "    pd.DataFrame.from_dict(queries, orient=\"index\", columns=[\"query\"])\n",
        "      .reset_index()\n",
        "      .rename(columns={\"index\": \"query_id\"})\n",
        ")\n",
        "\n",
        "df_queries"
      ],
      "id": "33099ccdf60f6a8b",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-01-12T02:56:24.215818Z",
          "start_time": "2026-01-12T02:56:24.193124Z"
        },
        "id": "6bf47915bc60901"
      },
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "for qid, docs in qrels.items():\n",
        "    for doc_id, rel in docs.items():\n",
        "        rows.append({\n",
        "            \"query_id\": qid,\n",
        "            \"doc_id\": doc_id,\n",
        "            \"relevance\": rel\n",
        "        })\n",
        "\n",
        "df_qrels = pd.DataFrame(rows)\n",
        "df_qrels"
      ],
      "id": "6bf47915bc60901",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-01-12T02:59:26.182045Z",
          "start_time": "2026-01-12T02:59:26.173041Z"
        },
        "id": "195f96b496255450"
      },
      "cell_type": "code",
      "source": [
        "# Elegimos una query cualquiera que tenga varios documentos relevantes\n",
        "qid = \"133\"\n",
        "\n",
        "print(\"Query:\")\n",
        "print(df_queries.loc[df_queries[\"query_id\"] == qid, \"query\"].values[0])\n",
        "\n",
        "print(\"\\nDocumentos relevantes para esta query:\")\n",
        "df_qrels[(df_qrels[\"query_id\"] == qid) & (df_qrels[\"relevance\"] > 0)]"
      ],
      "id": "195f96b496255450",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "e0623dc48a1e2500"
      },
      "cell_type": "markdown",
      "source": [
        "## Parte 2. Retrieval inicial (baseline)\n",
        "\n",
        "* Implementar retrieval inicial con BM25\n",
        "* Obtener métricas: Recall@10 nDCG@10"
      ],
      "id": "e0623dc48a1e2500"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25 nltk"
      ],
      "metadata": {
        "id": "WtUfpIPHkDd8"
      },
      "id": "WtUfpIPHkDd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_tokenize(text):\n",
        "    # Limpieza básica: minúsculas y quitar caracteres no alfanuméricos\n",
        "    text = re.sub(r'[^\\w\\s]', '', str(text).lower())\n",
        "    # Tokenización y Stemming (solo si no es stopword)\n",
        "    tokens = [stemmer.stem(w) for w in text.split() if w not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Aplicamos al corpus\n",
        "df_corpus['tokenized_text'] = df_corpus['text'].apply(clean_tokenize)\n",
        "bm25 = BM25Okapi(df_corpus['tokenized_text'].tolist())"
      ],
      "metadata": {
        "id": "kuPJWOUVmKSo"
      },
      "id": "kuPJWOUVmKSo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_list = []\n",
        "\n",
        "for _, q_row in df_queries.iterrows():\n",
        "    q_id = q_row['query_id']\n",
        "    q_text = q_row['query']\n",
        "\n",
        "    tokenized_query = clean_tokenize(q_text)\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "    # Obtenemos los índices de los top 10\n",
        "    top_n = 10\n",
        "    top_indices = scores.argsort()[-top_n:][::-1]\n",
        "\n",
        "    for rank, idx in enumerate(top_indices):\n",
        "        results_list.append({\n",
        "            'query_id': q_id,\n",
        "            'doc_id': df_corpus.iloc[idx]['doc_id'],\n",
        "            'score': scores[idx],\n",
        "            'rank': rank + 1\n",
        "        })\n",
        "\n",
        "df_results = pd.DataFrame(results_list)"
      ],
      "metadata": {
        "id": "jpPEvmdtmlBu"
      },
      "id": "jpPEvmdtmlBu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora se procede a evaluar las metricas Recall@10 y nDCG@10"
      ],
      "metadata": {
        "id": "ORd_oU8Em2hI"
      },
      "id": "ORd_oU8Em2hI"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ir_measures"
      ],
      "metadata": {
        "id": "vfzkFyD9nMhP"
      },
      "id": "vfzkFyD9nMhP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ir_measures\n",
        "from ir_measures import read_trec_run, nDCG, Recall\n",
        "\n",
        "# 1. Adaptar formatos (asegúrate de que los IDs sean strings)\n",
        "df_qrels = df_qrels.astype({'query_id': str, 'doc_id': str})\n",
        "df_results = df_results.astype({'query_id': str, 'doc_id': str})\n",
        "\n",
        "# 2. Calcular métricas\n",
        "metrics = ir_measures.calc_aggregate([Recall@10, nDCG@10], df_qrels, df_results)\n",
        "\n",
        "print(\"--- Resultados de la Evaluación ---\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "j-n9XnN2nBJl"
      },
      "id": "j-n9XnN2nBJl",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d61bce11043a6232"
      },
      "cell_type": "markdown",
      "source": [
        "## Parte 3. Implementación del re-ranking _cross-encoder_\n",
        "\n",
        "* Re-rankear los top-k candidatos para cada query.\n",
        "* Identificar qué documentos cambian de posición en el top 10"
      ],
      "id": "d61bce11043a6232"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "IIsT4s2JogKX"
      },
      "id": "IIsT4s2JogKX",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "234ab1a8794c9c3b"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Cargamos el modelo (puedes elegir uno específico para tu idioma)\n",
        "model_re_ranker = CrossEncoder('BAAI/bge-reranker-base')\n",
        "\n",
        "# Supongamos que re-rankeamos los top 50 de BM25 para mejorar el top 10 final\n",
        "top_k_bm25 = df_results.copy()"
      ],
      "id": "234ab1a8794c9c3b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proceso de Reranking"
      ],
      "metadata": {
        "id": "Toe3JQMnprrS"
      },
      "id": "Toe3JQMnprrS"
    },
    {
      "cell_type": "code",
      "source": [
        "reranked_data = []\n",
        "\n",
        "# Agrupamos los resultados de BM25 por query para procesarlos\n",
        "for query_id, group in top_k_bm25.groupby('query_id'):\n",
        "    query_text = df_queries.loc[df_queries['query_id'] == query_id, 'query'].values[0]\n",
        "\n",
        "    # Obtenemos los textos de los documentos candidatos (vía df_corpus)\n",
        "    doc_ids = group['doc_id'].tolist()\n",
        "    doc_texts = df_corpus[df_corpus['doc_id'].isin(doc_ids)].set_index('doc_id').loc[doc_ids, 'text'].tolist()\n",
        "\n",
        "    # Preparamos los pares para el Cross-Encoder\n",
        "    sentence_pairs = [[query_text, doc] for doc in doc_texts]\n",
        "\n",
        "    # Predicción de scores de relevancia\n",
        "    cross_scores = model_re_ranker.predict(sentence_pairs)\n",
        "\n",
        "    # Crear nuevos resultados\n",
        "    for i in range(len(doc_ids)):\n",
        "        reranked_data.append({\n",
        "            'query_id': query_id,\n",
        "            'doc_id': doc_ids[i],\n",
        "            'bm25_rank': group.iloc[i]['rank'], # Guardamos el rango anterior\n",
        "            'cross_score': cross_scores[i]\n",
        "        })\n",
        "\n",
        "df_reranked = pd.DataFrame(reranked_data)\n",
        "\n",
        "# Ordenar por el nuevo score y asignar nuevo rank\n",
        "df_reranked = df_reranked.sort_values(by=['query_id', 'cross_score'], ascending=[True, False])\n",
        "df_reranked['reranked_rank'] = df_reranked.groupby('query_id').cumcount() + 1"
      ],
      "metadata": {
        "id": "LDCdqHnLpyLP"
      },
      "id": "LDCdqHnLpyLP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se identifica cambios de posición en el Top 10"
      ],
      "metadata": {
        "id": "W3ziHy_IqMP-"
      },
      "id": "W3ziHy_IqMP-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtramos solo el nuevo Top 10\n",
        "top10_changes = df_reranked[df_reranked['reranked_rank'] <= 10].copy()\n",
        "\n",
        "# Calculamos el desplazamiento\n",
        "top10_changes['shift'] = top10_changes['bm25_rank'] - top10_changes['reranked_rank']\n",
        "\n",
        "def describe_change(row):\n",
        "    if row['shift'] > 0: return f\"Subió {int(row['shift'])} posiciones\"\n",
        "    if row['shift'] < 0: return f\"Bajó {int(abs(row['shift']))} posiciones\"\n",
        "    return \"Sin cambios\"\n",
        "\n",
        "top10_changes['status'] = top10_changes.apply(describe_change, axis=1)\n",
        "\n",
        "# Mostrar ejemplos donde hubo movimiento\n",
        "print(top10_changes[top10_changes['shift'] != 0][['query_id', 'doc_id', 'bm25_rank', 'reranked_rank', 'status']].head())"
      ],
      "metadata": {
        "id": "4uQyESrkqQYF"
      },
      "id": "4uQyESrkqQYF",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a7fcee9da8a7a263"
      },
      "cell_type": "markdown",
      "source": [
        "## Parte 4. Implementación del re-ranking _LTR_\n",
        "\n",
        "* Re-rankear los top-k candidatos para cada query.\n",
        "* Identificar qué documentos cambian de posición en el top 10"
      ],
      "id": "a7fcee9da8a7a263"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparación de Features"
      ],
      "metadata": {
        "id": "HZxNaha4uS_v"
      },
      "id": "HZxNaha4uS_v"
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Asegurémonos de rescatar el score de BM25 original\n",
        "# Creamos un dataframe limpio con los scores de BM25\n",
        "df_bm25_scores = df_results[['query_id', 'doc_id', 'score']].rename(columns={'score': 'score_bm25'})\n",
        "\n",
        "# 2. Unimos con los resultados del Cross-Encoder (df_reranked)\n",
        "# Usamos 'inner' para asegurarnos de tener ambos scores para cada par query-doc\n",
        "df_ltr = pd.merge(\n",
        "    df_reranked,\n",
        "    df_bm25_scores,\n",
        "    on=['query_id', 'doc_id'],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# 3. Unimos con las etiquetas reales (qrels) para el entrenamiento\n",
        "df_ltr = pd.merge(df_ltr, df_qrels, on=['query_id', 'doc_id'], how='left')\n",
        "df_ltr['relevance'] = df_ltr['relevance'].fillna(0)  # Los no encontrados en qrels son 0\n",
        "\n",
        "# 4. Ordenar por query_id es CRÍTICO para LTR\n",
        "df_ltr = df_ltr.sort_values('query_id')\n",
        "\n",
        "# 5. Ahora definimos las features con los nombres correctos\n",
        "# 'score_bm25' viene de BM25 y 'cross_score' viene del Cross-Encoder\n",
        "features = ['score_bm25', 'cross_score']\n",
        "X = df_ltr[features]\n",
        "y = df_ltr['relevance']\n",
        "groups = df_ltr.groupby('query_id').size().to_list()"
      ],
      "metadata": {
        "id": "NbFGEyX1vORn"
      },
      "id": "NbFGEyX1vORn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento e Identificación de Cambios"
      ],
      "metadata": {
        "id": "50BWhxaFvkkq"
      },
      "id": "50BWhxaFvkkq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el Ranker\n",
        "ranker = lgb.LGBMRanker(\n",
        "    objective=\"lambdarank\",\n",
        "    metric=\"ndcg\",\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "ranker.fit(X, y, group=groups)\n",
        "\n",
        "# Generar el score final de LTR\n",
        "df_ltr['ltr_score'] = ranker.predict(X)\n",
        "\n",
        "# Ranking final\n",
        "df_ltr = df_ltr.sort_values(by=['query_id', 'ltr_score'], ascending=[True, False])\n",
        "df_ltr['ltr_rank'] = df_ltr.groupby('query_id').cumcount() + 1\n",
        "\n",
        "# --- IDENTIFICAR CAMBIOS EN TOP 10 ---\n",
        "# Comparamos el rank inicial (BM25) con el final (LTR)\n",
        "top10_final = df_ltr[df_ltr['ltr_rank'] <= 10].copy()\n",
        "top10_final['change'] = top10_final['bm25_rank'] - top10_final['ltr_rank']\n",
        "\n",
        "print(\"Resumen de movimientos (Documentos que más subieron gracias a LTR):\")\n",
        "print(top10_final[['query_id', 'doc_id', 'bm25_rank', 'ltr_rank', 'change']].sort_values('change', ascending=False).head(10))"
      ],
      "metadata": {
        "id": "hYWslt9DvjpM"
      },
      "id": "hYWslt9DvjpM",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c3ea1872bc57044b"
      },
      "cell_type": "markdown",
      "source": [
        "## Parte 5. Evaluación post re-ranking\n",
        "\n",
        "Calcular métricas:\n",
        "* nDCG@10\n",
        "* MAP\n",
        "* Recall@10"
      ],
      "id": "c3ea1872bc57044b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparación de los datos para la Evaluación"
      ],
      "metadata": {
        "id": "G-JdB-FhxLmg"
      },
      "id": "G-JdB-FhxLmg"
    },
    {
      "metadata": {
        "id": "1519eb02cfc91afb"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import ir_measures\n",
        "from ir_measures import nDCG, MAP, Recall\n",
        "\n",
        "# 1. Aseguramos que los IDs sean strings para evitar errores de comparación\n",
        "df_qrels_eval = df_qrels.astype({'query_id': str, 'doc_id': str})\n",
        "\n",
        "# 2. Preparamos el DataFrame de LTR (el ranking final)\n",
        "# Renombramos 'ltr_score' a 'score' porque es lo que busca la librería\n",
        "df_ltr_eval = df_ltr[['query_id', 'doc_id', 'ltr_score']].copy()\n",
        "df_ltr_eval.columns = ['query_id', 'doc_id', 'score']\n",
        "df_ltr_eval = df_ltr_eval.astype({'query_id': str, 'doc_id': str})\n",
        "\n",
        "# 3. Preparamos también el ranking de Cross-Encoder y BM25 para comparar\n",
        "df_ce_eval = df_reranked[['query_id', 'doc_id', 'cross_score']].copy()\n",
        "df_ce_eval.columns = ['query_id', 'doc_id', 'score']\n",
        "df_ce_eval = df_ce_eval.astype({'query_id': str, 'doc_id': str})\n",
        "\n",
        "df_bm25_eval = df_results[['query_id', 'doc_id', 'score']].copy()\n",
        "df_bm25_eval = df_bm25_eval.astype({'query_id': str, 'doc_id': str})"
      ],
      "id": "1519eb02cfc91afb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cálculo de Métricas Comparativas"
      ],
      "metadata": {
        "id": "TqMCmjqkxUze"
      },
      "id": "TqMCmjqkxUze"
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos las métricas deseadas\n",
        "metrics_to_run = [nDCG@10, MAP, Recall@10]\n",
        "\n",
        "# Calculamos para cada etapa\n",
        "results_bm25 = ir_measures.calc_aggregate(metrics_to_run, df_qrels_eval, df_bm25_eval)\n",
        "results_ce   = ir_measures.calc_aggregate(metrics_to_run, df_qrels_eval, df_ce_eval)\n",
        "results_ltr  = ir_measures.calc_aggregate(metrics_to_run, df_qrels_eval, df_ltr_eval)\n",
        "\n",
        "# Formateamos los resultados en una tabla comparativa\n",
        "eval_df = pd.DataFrame([results_bm25, results_ce, results_ltr],\n",
        "                       index=['BM25 (Base)', 'Cross-Encoder (Re-rank)', 'LTR (Final)'])\n",
        "\n",
        "print(\"--- REPORTE FINAL DE MÉTRICAS ---\")\n",
        "print(eval_df.round(4))"
      ],
      "metadata": {
        "id": "CyG9boBJxVNE"
      },
      "id": "CyG9boBJxVNE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}